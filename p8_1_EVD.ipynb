{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征值分解\n",
    "# 参考：https://www.bilibili.com/video/BV1TH4y1L7PV/ 漫士沉思录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征值与特征向量究竟是什么含义？\n",
    "\n",
    "# 把点x看成原坐标系的一个向量， 当矩阵A作用于x时，得到原坐标系的新的向量x'\n",
    "# 当在原坐标系中，x与x'在同一条直线上，则称这条直线上的向量为特征向量\n",
    "# 这条直线上所有向量都缩放一样的倍数，称之为特征值\n",
    "# 找到A矩阵所有的特征值与特征向量，比如说A为2×2的矩阵，则最多有两个特征值\n",
    "# 这时候如果不知道A矩阵本身，只知道特征值和特征向量所在的直线，那么一个任一点x，能够得到变换后的新的点x'的坐标吗？\n",
    "# 答案是可以的，只需要把x，在特征向量所在的直线分解，然后分别缩放特征值倍，把缩放后的向量再合成，就可以得到x'的坐标了\n",
    "# 所以：特征值和特征向量反映了A矩阵的空间变换性质"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为什么要把矩阵A分解成：A=PΛP^−1\n",
    "# We concatenate all eigenvectors to form a matrix P = (v1, . . . , vN )，P的列向量是相互独立的，因为还有求解它的逆\n",
    "# We form all eigenvalues into a diagonal matrix Λ = diag(λ1, . . . , λN )^T\n",
    "# (remark: by convention, we typically sort the eigenvalues in descending order)\n",
    "\n",
    "# 1、V^−1相当于把原坐标系的点，表示为特征向量构成的坐标系\n",
    "# 2、在特征向量为基的坐标系中，对其进行缩放\n",
    "# 3、V相当于再还原到原来的坐标系，得到最终变换后的点在原坐标系的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好处：\n",
    "比如说A^2，可以写成：\n",
    "$$A^{2} = P\\Lambda P^{-1}  P\\Lambda P^{-1}  =P\\Lambda^{2}P^{-1}$$\n",
    "进而得到：\n",
    "$$A^{n} = =P\\Lambda^{n}P^{-1}$$\n",
    "让矩阵连续相乘计算变得简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值：\n",
      "tensor([5.0000+0.j, 2.0000+0.j])\n",
      "特征向量：\n",
      "tensor([[ 0.8944+0.j, -0.7071+0.j],\n",
      "        [ 0.4472+0.j,  0.7071+0.j]])\n",
      "对角特征值矩阵 Λ：\n",
      "tensor([[5.0000+0.j, 0.0000+0.j],\n",
      "        [0.0000+0.j, 2.0000+0.j]])\n",
      "重构的 A：\n",
      "tensor([[4.0000+0.j, 2.0000+0.j],\n",
      "        [1.0000+0.j, 3.0000+0.j]])\n"
     ]
    }
   ],
   "source": [
    "# 用pytorch实现矩阵的特征值分解\n",
    "import torch\n",
    "\n",
    "# 创建一个方阵 A\n",
    "A = torch.tensor([[4.0, 2.0],\n",
    "                  [1.0, 3.0]])\n",
    "\n",
    "# 进行特征值分解\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(A)\n",
    "\n",
    "# 构造对角矩阵 Λ\n",
    "Lambda = torch.diag(eigenvalues)\n",
    "\n",
    "# 计算 P 的逆\n",
    "P_inverse = torch.linalg.inv(eigenvectors)\n",
    "\n",
    "# 验证 A = PΛP^−1\n",
    "A_reconstructed = eigenvectors @ Lambda @ P_inverse\n",
    "\n",
    "# 输出结果\n",
    "print(\"特征值：\")\n",
    "print(eigenvalues)\n",
    "\n",
    "print(\"特征向量：\")\n",
    "print(eigenvectors)\n",
    "\n",
    "print(\"对角特征值矩阵 Λ：\")\n",
    "print(Lambda)\n",
    "\n",
    "print(\"重构的 A：\")\n",
    "print(A_reconstructed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
